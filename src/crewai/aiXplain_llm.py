import logging
from crewai.llm import LLM
from typing import Any, Dict, List, Optional

from aixplain.factories import ModelFactory
from aixplain.enums import Function


class AixplainLLM(LLM):
    """
    A wrapper class to integrate aiXplain's LLM models with the existing agent framework.
    """
    

    def __init__(
        self,
        model_id: str,
        team_api_key: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        **kwargs,
    ):
        """
        Initialize the aiXplain LLM wrapper.

        Args:
            model_id (str): The ID of the aiXplain model.
            team_api_key (str, optional): API key for the aiXplain team (if required).
            temperature (float, optional): Sampling temperature for text generation.
            max_tokens (int, optional): Maximum number of tokens for the generated text.
            **kwargs: Additional parameters to be passed to the model.
        """
        super().__init__(model_id)
        self.model_id = model_id
        self.team_api_key = team_api_key
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.kwargs = kwargs

        # Initialize the aiXplain model
        self.model = ModelFactory.get(model_id)

        # Validate that the model supports text generation
        # assert (
        #     self.model.function == Function.TEXT_GENERATION
        # ), "Please select a text generation model."

    def call(self, messages: List[Dict[str, str]], callbacks: List[Any] = []) -> str:
        """
        Call the aiXplain LLM model with a list of messages and return the response.

        Args:
            messages (List[Dict[str, str]]): List of messages to be processed by the LLM.

        Returns:
            str: The response text generated by the model.
        """
        try:
            # Concatenate message contents into a single input string
            input_text = " ".join([message["content"] for message in messages])

            # Prepare input for the model
            inputs = {"text": input_text}

            # Add optional parameters if provided
            if self.temperature is not None:
                inputs["temperature"] = self.temperature
            if self.max_tokens is not None:
                inputs["max_tokens"] = self.max_tokens

            # Include any additional parameters from kwargs
            inputs.update(self.kwargs)

            # Run the model and get the result
            result = self.model.run(inputs)
            return result["data"]
        except Exception as e:
            logging.error(f"aiXplain LLM call failed: {str(e)}")
            raise

    def supports_function_calling(self) -> bool:
        """
        Return whether the LLM supports function calling (not applicable for aiXplain at the moment).

        Returns:
            bool: Always False, as aiXplain does not support function calling.
        """
        return False

    def supports_stop_words(self) -> bool:
        """
        Return whether the LLM supports stop words (not applicable for aiXplain at the moment).

        Returns:
            bool: Always False, as aiXplain does not support stop words.
        """
        return False

    def get_context_window_size(self) -> int:
        """
        Return the context window size for aiXplain models.

        Returns:
            int: Default context window size.
        """
        return 8192  
